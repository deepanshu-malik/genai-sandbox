{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finance Domain RAG System\n",
    "\n",
    "Interactive notebook for exploring RAG with finance domain documents:\n",
    "- **LOS** (Loan Origination System)\n",
    "- **LMS** (Loan Management System)  \n",
    "- **Credit Reports Analysis**\n",
    "- **Underwriting Guidelines**\n",
    "\n",
    "## Workflow\n",
    "1. **Setup** (Run once)\n",
    "2. **Ingest Documents** (Run once - creates embeddings)\n",
    "3. **Query the System** (Run multiple times with different questions)\n",
    "\n",
    "This saves embedding costs while you experiment! üí∞"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Setup complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import chromadb\n",
    "from pathlib import Path\n",
    "import re\n",
    "from typing import List, Dict\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Helper functions defined!\n"
     ]
    }
   ],
   "source": [
    "def get_embedding(text: str) -> list[float]:\n",
    "    \"\"\"Convert text to embedding vector\"\"\"\n",
    "    response = client.embeddings.create(\n",
    "        model=\"text-embedding-3-small\",\n",
    "        input=text\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "\n",
    "def chunk_document(text: str, sentences_per_chunk: int = 4, overlap_sentences: int = 1) -> List[str]:\n",
    "    \"\"\"Chunk text by sentences with overlap\"\"\"\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n",
    "    chunks = []\n",
    "    i = 0\n",
    "\n",
    "    while i < len(sentences):\n",
    "        chunk_sentences = sentences[i:i + sentences_per_chunk]\n",
    "        chunk_text = ' '.join(chunk_sentences)\n",
    "        if chunk_text.strip():\n",
    "            chunks.append(chunk_text)\n",
    "        i += sentences_per_chunk - overlap_sentences\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def print_retrieved_chunks(chunks: List[Dict]):\n",
    "    \"\"\"Pretty print retrieved chunks\"\"\"\n",
    "    print(\"\\nüìä RETRIEVED CHUNKS:\")\n",
    "    print(\"=\" * 100)\n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        print(f\"\\n#{i} - [{chunk['title']}] (Similarity: {chunk['similarity']:.3f})\")\n",
    "        print(f\"Document Type: {chunk['document_type']}\")\n",
    "        print(\"-\" * 100)\n",
    "        print(f\"{chunk['text'][:300]}...\" if len(chunk['text']) > 300 else chunk['text'])\n",
    "\n",
    "\n",
    "print(\"‚úÖ Helper functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Initialize ChromaDB & Ingest Documents\n",
    "\n",
    "**RUN THIS ONCE** - Creates embeddings for all finance documents.\n",
    "\n",
    "‚ö†Ô∏è This costs tokens (~$0.001 for all documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Finance RAG system...\n",
      "‚úÖ ChromaDB initialized\n",
      "\n",
      "üìö Found 4 finance documents:\n",
      "\n",
      "  üìÑ Credit Reports\n",
      "  üìÑ Loan Management System\n",
      "  üìÑ Loan Origination System\n",
      "  üìÑ Underwriting Guidelines\n",
      "\n",
      "====================================================================================================\n",
      "INGESTING DOCUMENTS (Creating embeddings - this costs tokens!)\n",
      "====================================================================================================\n",
      "\n",
      "üìÑ Credit Reports\n",
      "   ‚Üí 15 chunks created\n",
      "üìÑ Loan Management System\n",
      "   ‚Üí 12 chunks created\n",
      "üìÑ Loan Origination System\n",
      "   ‚Üí 11 chunks created\n",
      "üìÑ Underwriting Guidelines\n",
      "   ‚Üí 18 chunks created\n",
      "\n",
      "====================================================================================================\n",
      "‚úÖ INGESTION COMPLETE!\n",
      "====================================================================================================\n",
      "üìä Total chunks: 56\n",
      "üí∞ Embedding cost: ~$0.001120\n",
      "üíæ Saved to: ./finance_notebook_db\n",
      "\n",
      "üéØ Now you can query unlimited times in Cell 4 without additional embedding costs!\n"
     ]
    }
   ],
   "source": [
    "# Initialize ChromaDB\n",
    "print(\"Initializing Finance RAG system...\")\n",
    "chroma_client = chromadb.PersistentClient(path=\"./finance_notebook_db\")\n",
    "\n",
    "# Delete existing collection (for clean start)\n",
    "try:\n",
    "    chroma_client.delete_collection(name=\"finance_kb\")\n",
    "    print(\"üóëÔ∏è  Cleared existing knowledge base\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Create new collection\n",
    "collection = chroma_client.create_collection(\n",
    "    name=\"finance_kb\",\n",
    "    metadata={\"description\": \"Finance domain knowledge base\"}\n",
    ")\n",
    "\n",
    "print(\"‚úÖ ChromaDB initialized\\n\")\n",
    "\n",
    "# Ingest finance documents\n",
    "doc_directory = \"./sample_docs/finance\"\n",
    "doc_path = Path(doc_directory)\n",
    "doc_files = list(doc_path.glob(\"*.txt\"))\n",
    "\n",
    "print(f\"üìö Found {len(doc_files)} finance documents:\\n\")\n",
    "for f in doc_files:\n",
    "    print(f\"  üìÑ {f.stem.replace('_', ' ').title()}\")\n",
    "\n",
    "print(f\"\\n{'='*100}\")\n",
    "print(\"INGESTING DOCUMENTS (Creating embeddings - this costs tokens!)\")\n",
    "print(f\"{'='*100}\\n\")\n",
    "\n",
    "all_ids = []\n",
    "all_chunks = []\n",
    "all_embeddings = []\n",
    "all_metadata = []\n",
    "chunk_counter = 0\n",
    "\n",
    "for doc_file in doc_files:\n",
    "    title = doc_file.stem.replace('_', ' ').title()\n",
    "    content = doc_file.read_text()\n",
    "\n",
    "    # Chunk the document\n",
    "    chunks = chunk_document(content, sentences_per_chunk=4, overlap_sentences=1)\n",
    "\n",
    "    print(f\"üìÑ {title}\")\n",
    "    print(f\"   ‚Üí {len(chunks)} chunks created\")\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk_id = f\"{doc_file.stem}_chunk_{i}\"\n",
    "        embedding = get_embedding(chunk)\n",
    "\n",
    "        all_ids.append(chunk_id)\n",
    "        all_chunks.append(chunk)\n",
    "        all_embeddings.append(embedding)\n",
    "        all_metadata.append({\n",
    "            \"title\": title,\n",
    "            \"category\": \"finance\",\n",
    "            \"document_type\": doc_file.stem,\n",
    "            \"chunk_index\": i,\n",
    "            \"total_chunks\": len(chunks)\n",
    "        })\n",
    "\n",
    "        chunk_counter += 1\n",
    "\n",
    "# Batch add to collection\n",
    "collection.add(\n",
    "    ids=all_ids,\n",
    "    documents=all_chunks,\n",
    "    embeddings=all_embeddings,\n",
    "    metadatas=all_metadata\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*100}\")\n",
    "print(f\"‚úÖ INGESTION COMPLETE!\")\n",
    "print(f\"{'='*100}\")\n",
    "print(f\"üìä Total chunks: {chunk_counter}\")\n",
    "print(f\"üí∞ Embedding cost: ~${chunk_counter * 0.00002:.6f}\")\n",
    "print(f\"üíæ Saved to: ./finance_notebook_db\")\n",
    "print(f\"\\nüéØ Now you can query unlimited times in Cell 4 without additional embedding costs!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Query the System (Run Multiple Times!)\n",
    "\n",
    "**Change the `question` variable and re-run** to explore different queries.\n",
    "\n",
    "No additional embedding costs - we already have everything stored! üéâ\n",
    "\n",
    "### Try These Questions:\n",
    "- \"How does a Loan Origination System handle income verification?\"\n",
    "- \"What is debt-to-credit ratio and how does it affect credit scores?\"\n",
    "- \"How does an LMS handle delinquent loans?\"\n",
    "- \"What are the DTI requirements for conventional mortgages?\"\n",
    "- \"How do credit scores impact loan approval?\"\n",
    "- \"What is the difference between hard and soft credit inquiries?\"\n",
    "- \"How are escrow accounts managed in loan servicing?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "‚ùì QUESTION: How does a Loan Origination System handle income verification?\n",
      "====================================================================================================\n",
      "\n",
      "üîç Retrieving top 3 relevant chunks...\n",
      "\n",
      "\n",
      "üìä RETRIEVED CHUNKS:\n",
      "====================================================================================================\n",
      "\n",
      "#1 - [Loan Origination System] (Similarity: 0.596)\n",
      "Document Type: loan_origination_system\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Modern LOS platforms integrate with services like The Work Number and Equifax to verify employment and income electronically. For self-employed borrowers, the system can analyze bank statements using AI to calculate qualifying income. Asset verification follows similar automation. The LOS can connec...\n",
      "\n",
      "#2 - [Loan Origination System] (Similarity: 0.554)\n",
      "Document Type: loan_origination_system\n",
      "----------------------------------------------------------------------------------------------------\n",
      "It calculates the borrower's debt-to-income ratio (DTI) and compares it against lending guidelines. Most conventional loans require a DTI below 43 percent. Income verification has become increasingly automated. Modern LOS platforms integrate with services like The Work Number and Equifax to verify e...\n",
      "\n",
      "#3 - [Loan Origination System] (Similarity: 0.553)\n",
      "Document Type: loan_origination_system\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Loan Origination System (LOS) Overview\n",
      "\n",
      "A Loan Origination System is a software platform that automates the entire loan application and approval process from initial application to funding. Modern LOS platforms integrate with credit bureaus, income verification services, and document management syst...\n",
      "\n",
      "====================================================================================================\n",
      "ü§ñ GENERATING ANSWER...\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "üí° ANSWER:\n",
      "====================================================================================================\n",
      "A Loan Origination System (LOS) handles income verification through automation and integration with external services. It utilizes platforms like The Work Number and Equifax to electronically verify employment and income. For self-employed borrowers, the LOS can analyze bank statements using artificial intelligence to calculate qualifying income. This automation streamlines the income verification process, ensuring efficiency and accuracy in assessing a borrower's financial capability (Source 1: Loan Origination System; Source 2: Loan Origination System).\n",
      "\n",
      "====================================================================================================\n",
      "üìö Sources: Loan Origination System\n",
      "üìä Tokens used: 427 | Cost: $0.000064\n",
      "====================================================================================================\n",
      "\n",
      "‚ú® Try changing the question above and re-running this cell!\n"
     ]
    }
   ],
   "source": [
    "# üîß CHANGE THIS QUESTION AND RE-RUN!\n",
    "question = \"How does a Loan Origination System handle income verification?\"\n",
    "\n",
    "# Number of chunks to retrieve (adjust based on question complexity)\n",
    "n_results = 3\n",
    "\n",
    "print(f\"{'='*100}\")\n",
    "print(f\"‚ùì QUESTION: {question}\")\n",
    "print(f\"{'='*100}\")\n",
    "\n",
    "# Step 1: Retrieve relevant chunks\n",
    "print(f\"\\nüîç Retrieving top {n_results} relevant chunks...\\n\")\n",
    "\n",
    "query_embedding = get_embedding(question)\n",
    "\n",
    "results = collection.query(\n",
    "    query_embeddings=[query_embedding],\n",
    "    n_results=n_results,\n",
    "    include=[\"documents\", \"metadatas\", \"distances\"]\n",
    ")\n",
    "\n",
    "# Format results\n",
    "retrieved_chunks = []\n",
    "for doc, meta, dist in zip(results[\"documents\"][0], results[\"metadatas\"][0], results[\"distances\"][0]):\n",
    "    similarity = 1 / (1 + dist)\n",
    "    retrieved_chunks.append({\n",
    "        \"text\": doc,\n",
    "        \"title\": meta[\"title\"],\n",
    "        \"document_type\": meta[\"document_type\"],\n",
    "        \"similarity\": similarity\n",
    "    })\n",
    "\n",
    "# Display retrieved chunks\n",
    "print_retrieved_chunks(retrieved_chunks)\n",
    "\n",
    "# Step 2: Generate answer with context\n",
    "print(f\"\\n{'='*100}\")\n",
    "print(\"ü§ñ GENERATING ANSWER...\")\n",
    "print(f\"{'='*100}\\n\")\n",
    "\n",
    "# Build context from retrieved chunks\n",
    "context_parts = []\n",
    "sources = set()\n",
    "\n",
    "for i, chunk in enumerate(retrieved_chunks, 1):\n",
    "    context_parts.append(f\"[Source {i}: {chunk['title']}]\\n{chunk['text']}\\n\")\n",
    "    sources.add(chunk['title'])\n",
    "\n",
    "context = \"\\n\".join(context_parts)\n",
    "\n",
    "# Construct prompt\n",
    "prompt = f\"\"\"You are a finance domain expert assistant specializing in lending systems, credit analysis, and loan processing.\n",
    "\n",
    "CONTEXT FROM KNOWLEDGE BASE:\n",
    "{context}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Answer the question using ONLY the information in the context above\n",
    "- If the context doesn't contain enough information, say so clearly\n",
    "- Cite your sources by mentioning the document titles\n",
    "- Be precise with financial terms and regulatory requirements\n",
    "- Use industry terminology appropriately\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "ANSWER:\"\"\"\n",
    "\n",
    "# Call LLM\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    temperature=0.2,  # Low temperature for factual finance answers\n",
    ")\n",
    "\n",
    "answer = response.choices[0].message.content\n",
    "tokens_used = response.usage.total_tokens\n",
    "cost = tokens_used * 0.00000015  # gpt-4o-mini pricing\n",
    "\n",
    "# Display answer\n",
    "print(f\"{'='*100}\")\n",
    "print(\"üí° ANSWER:\")\n",
    "print(f\"{'='*100}\")\n",
    "print(answer)\n",
    "print(f\"\\n{'='*100}\")\n",
    "print(f\"üìö Sources: {', '.join(sources)}\")\n",
    "print(f\"üìä Tokens used: {tokens_used} | Cost: ${cost:.6f}\")\n",
    "print(f\"{'='*100}\")\n",
    "\n",
    "print(\"\\n‚ú® Try changing the question above and re-running this cell!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Explore the Knowledge Base\n",
    "\n",
    "See what documents and chunks are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "üìä KNOWLEDGE BASE STATISTICS\n",
      "====================================================================================================\n",
      "Total chunks: 56\n",
      "Storage location: ./finance_notebook_db\n",
      "\n",
      "====================================================================================================\n",
      "üìÑ SAMPLE CHUNKS BY DOCUMENT TYPE\n",
      "====================================================================================================\n",
      "\n",
      "üìÑ Loan Origination System\n",
      "   Total chunks: 11\n",
      "   Sample: Loan Origination System (LOS) Overview\n",
      "\n",
      "A Loan Origination System is a software platform that automates the entire loan application and approval proce...\n",
      "\n",
      "üìÑ Loan Management System\n",
      "   Total chunks: 12\n",
      "   Sample: Loan Management System (LMS) Fundamentals\n",
      "\n",
      "A Loan Management System is the core platform that handles all post-origination loan activities from first ...\n",
      "\n",
      "üìÑ Credit Reports\n",
      "   Total chunks: 15\n",
      "   Sample: Credit Report Analysis and Interpretation\n",
      "\n",
      "Credit reports are comprehensive records of an individual's credit history maintained by three major credit...\n",
      "\n",
      "üìÑ Underwriting Guidelines\n",
      "   Total chunks: 18\n",
      "   Sample: Mortgage Underwriting Guidelines and Standards\n",
      "\n",
      "Underwriting is the process of evaluating a borrower's creditworthiness and the property's value to de...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Collection stats\n",
    "total_chunks = collection.count()\n",
    "\n",
    "print(f\"{'='*100}\")\n",
    "print(\"üìä KNOWLEDGE BASE STATISTICS\")\n",
    "print(f\"{'='*100}\")\n",
    "print(f\"Total chunks: {total_chunks}\")\n",
    "print(f\"Storage location: ./finance_notebook_db\")\n",
    "\n",
    "# Get sample chunks from each document type\n",
    "print(f\"\\n{'='*100}\")\n",
    "print(\"üìÑ SAMPLE CHUNKS BY DOCUMENT TYPE\")\n",
    "print(f\"{'='*100}\\n\")\n",
    "\n",
    "doc_types = [\"loan_origination_system\", \"loan_management_system\", \"credit_reports\", \"underwriting_guidelines\"]\n",
    "\n",
    "for doc_type in doc_types:\n",
    "    results = collection.get(\n",
    "        where={\"document_type\": doc_type},\n",
    "        limit=1,\n",
    "        include=[\"documents\", \"metadatas\"]\n",
    "    )\n",
    "    \n",
    "    if results['documents']:\n",
    "        meta = results['metadatas'][0]\n",
    "        doc = results['documents'][0]\n",
    "        \n",
    "        print(f\"üìÑ {meta['title']}\")\n",
    "        print(f\"   Total chunks: {meta['total_chunks']}\")\n",
    "        print(f\"   Sample: {doc[:150]}...\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Compare Different Retrieval Strategies\n",
    "\n",
    "Experiment with different numbers of chunks retrieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing question: 'What are the key components of underwriting?'\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "üìä Retrieving TOP 1 chunks:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "  1. [Underwriting Guidelines] - Similarity: 0.590\n",
      "\n",
      "üìä Retrieving TOP 3 chunks:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "  1. [Underwriting Guidelines] - Similarity: 0.590\n",
      "  2. [Underwriting Guidelines] - Similarity: 0.509\n",
      "  3. [Underwriting Guidelines] - Similarity: 0.504\n",
      "\n",
      "üìä Retrieving TOP 5 chunks:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "  1. [Underwriting Guidelines] - Similarity: 0.590\n",
      "  2. [Underwriting Guidelines] - Similarity: 0.509\n",
      "  3. [Underwriting Guidelines] - Similarity: 0.504\n",
      "  4. [Underwriting Guidelines] - Similarity: 0.498\n",
      "  5. [Loan Origination System] - Similarity: 0.490\n",
      "\n",
      "====================================================================================================\n",
      "üí° OBSERVATION:\n",
      "More chunks = more context but also more noise\n",
      "Sweet spot is usually 3-5 chunks for most questions\n"
     ]
    }
   ],
   "source": [
    "test_question = \"What are the key components of underwriting?\"\n",
    "\n",
    "print(f\"Testing question: '{test_question}'\\n\")\n",
    "print(f\"{'='*100}\\n\")\n",
    "\n",
    "for n in [1, 3, 5]:\n",
    "    print(f\"üìä Retrieving TOP {n} chunks:\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    query_embedding = get_embedding(test_question)\n",
    "    \n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=n,\n",
    "        include=[\"metadatas\", \"distances\"]\n",
    "    )\n",
    "    \n",
    "    for i, (meta, dist) in enumerate(zip(results[\"metadatas\"][0], results[\"distances\"][0]), 1):\n",
    "        similarity = 1 / (1 + dist)\n",
    "        print(f\"  {i}. [{meta['title']}] - Similarity: {similarity:.3f}\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "print(f\"{'='*100}\")\n",
    "print(\"üí° OBSERVATION:\")\n",
    "print(\"More chunks = more context but also more noise\")\n",
    "print(\"Sweet spot is usually 3-5 chunks for most questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Test Edge Cases\n",
    "\n",
    "What happens when we ask questions NOT in the knowledge base?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùì Out-of-scope question: 'How do I deploy a React application to AWS?'\n",
      "\n",
      "üìä Best matches (even though irrelevant):\n",
      "====================================================================================================\n",
      "\n",
      "1. [Loan Management System] - Similarity: 0.362\n",
      "   Customer self-service portals integrated with the LMS allow borrowers to view loan details, make payments, request payoff quotes, and access tax docum...\n",
      "\n",
      "2. [Loan Management System] - Similarity: 0.362\n",
      "   Portfolio dashboards show delinquency trends, prepayment rates, and risk metrics. Performance analytics help identify process improvements and benchma...\n",
      "\n",
      "3. [Loan Origination System] - Similarity: 0.361\n",
      "   Modern LOS platforms integrate with services like The Work Number and Equifax to verify employment and income electronically. For self-employed borrow...\n",
      "\n",
      "====================================================================================================\n",
      "üí° IMPORTANT: Notice the similarity scores are much lower!\n",
      "In production, set a similarity threshold (e.g., 0.4)\n",
      "If all results are below threshold, return 'I don't have information about that'\n"
     ]
    }
   ],
   "source": [
    "# Question NOT in knowledge base\n",
    "out_of_scope_question = \"How do I deploy a React application to AWS?\"\n",
    "\n",
    "print(f\"‚ùì Out-of-scope question: '{out_of_scope_question}'\\n\")\n",
    "\n",
    "query_embedding = get_embedding(out_of_scope_question)\n",
    "\n",
    "results = collection.query(\n",
    "    query_embeddings=[query_embedding],\n",
    "    n_results=3,\n",
    "    include=[\"documents\", \"metadatas\", \"distances\"]\n",
    ")\n",
    "\n",
    "print(\"üìä Best matches (even though irrelevant):\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "for i, (doc, meta, dist) in enumerate(zip(results[\"documents\"][0], results[\"metadatas\"][0], results[\"distances\"][0]), 1):\n",
    "    similarity = 1 / (1 + dist)\n",
    "    print(f\"\\n{i}. [{meta['title']}] - Similarity: {similarity:.3f}\")\n",
    "    print(f\"   {doc[:150]}...\")\n",
    "\n",
    "print(f\"\\n{'='*100}\")\n",
    "print(\"üí° IMPORTANT: Notice the similarity scores are much lower!\")\n",
    "print(\"In production, set a similarity threshold (e.g., 0.4)\")\n",
    "print(\"If all results are below threshold, return 'I don't have information about that'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Key Takeaways\n",
    "\n",
    "### What You've Learned:\n",
    "\n",
    "1. **Domain-Specific RAG**: Finance documents (LOS, LMS, Credit Reports, Underwriting) can be queried semantically\n",
    "\n",
    "2. **Cost Efficiency**: Embed once (Cell 3), query unlimited times (Cell 4) - saves money!\n",
    "\n",
    "3. **Semantic Search**: Finds relevant information even with different terminology\n",
    "   - Ask about \"vacation\" ‚Üí finds \"PTO\"\n",
    "   - Ask about \"DTI\" ‚Üí finds \"debt-to-income ratio\"\n",
    "\n",
    "4. **Source Attribution**: Every answer cites sources for auditability (critical in finance!)\n",
    "\n",
    "5. **Retrieval Strategies**: \n",
    "   - More chunks = more context but potential noise\n",
    "   - Sweet spot: 3-5 chunks\n",
    "   - Use similarity thresholds to detect out-of-scope questions\n",
    "\n",
    "### Production Considerations for Finance:\n",
    "\n",
    "- **Compliance**: Audit logs for all queries (who asked what, when)\n",
    "- **Version Control**: Track which version of regulations/policies was used\n",
    "- **Metadata Filtering**: Filter by effective date, regulation type, jurisdiction\n",
    "- **Confidence Scores**: Flag low-confidence answers for human review\n",
    "- **Access Control**: Role-based access to different document types\n",
    "- **Regulatory Updates**: Easy document refresh without system changes\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. Add your own finance documents (replace sample_docs/finance)\n",
    "2. Experiment with chunk sizes (Cell 3: `sentences_per_chunk`)\n",
    "3. Try different retrieval amounts (Cell 4: `n_results`)\n",
    "4. Implement metadata filtering for your use case\n",
    "5. Build a production API with FastAPI\n",
    "\n",
    "üéØ **You now understand RAG for Finance domain!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
