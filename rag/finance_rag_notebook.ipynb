{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finance Domain RAG System\n",
    "\n",
    "Interactive notebook for exploring RAG with finance domain documents:\n",
    "- **LOS** (Loan Origination System)\n",
    "- **LMS** (Loan Management System)  \n",
    "- **Credit Reports Analysis**\n",
    "- **Underwriting Guidelines**\n",
    "\n",
    "## Workflow\n",
    "1. **Setup** (Run once)\n",
    "2. **Ingest Documents** (Run once - creates embeddings)\n",
    "3. **Query the System** (Run multiple times with different questions)\n",
    "\n",
    "This saves embedding costs while you experiment! üí∞"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import chromadb\n",
    "from pathlib import Path\n",
    "import re\n",
    "from typing import List, Dict\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text: str) -> list[float]:\n",
    "    \"\"\"Convert text to embedding vector\"\"\"\n",
    "    response = client.embeddings.create(\n",
    "        model=\"text-embedding-3-small\",\n",
    "        input=text\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "\n",
    "def chunk_document(text: str, sentences_per_chunk: int = 4, overlap_sentences: int = 1) -> List[str]:\n",
    "    \"\"\"Chunk text by sentences with overlap\"\"\"\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n",
    "    chunks = []\n",
    "    i = 0\n",
    "\n",
    "    while i < len(sentences):\n",
    "        chunk_sentences = sentences[i:i + sentences_per_chunk]\n",
    "        chunk_text = ' '.join(chunk_sentences)\n",
    "        if chunk_text.strip():\n",
    "            chunks.append(chunk_text)\n",
    "        i += sentences_per_chunk - overlap_sentences\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def print_retrieved_chunks(chunks: List[Dict]):\n",
    "    \"\"\"Pretty print retrieved chunks\"\"\"\n",
    "    print(\"\\nüìä RETRIEVED CHUNKS:\")\n",
    "    print(\"=\" * 100)\n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        print(f\"\\n#{i} - [{chunk['title']}] (Similarity: {chunk['similarity']:.3f})\")\n",
    "        print(f\"Document Type: {chunk['document_type']}\")\n",
    "        print(\"-\" * 100)\n",
    "        print(f\"{chunk['text'][:300]}...\" if len(chunk['text']) > 300 else chunk['text'])\n",
    "\n",
    "\n",
    "print(\"‚úÖ Helper functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Initialize ChromaDB & Ingest Documents\n",
    "\n",
    "**RUN THIS ONCE** - Creates embeddings for all finance documents.\n",
    "\n",
    "‚ö†Ô∏è This costs tokens (~$0.001 for all documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ChromaDB\n",
    "print(\"Initializing Finance RAG system...\")\n",
    "chroma_client = chromadb.PersistentClient(path=\"./finance_notebook_db\")\n",
    "\n",
    "# Delete existing collection (for clean start)\n",
    "try:\n",
    "    chroma_client.delete_collection(name=\"finance_kb\")\n",
    "    print(\"üóëÔ∏è  Cleared existing knowledge base\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Create new collection\n",
    "collection = chroma_client.create_collection(\n",
    "    name=\"finance_kb\",\n",
    "    metadata={\"description\": \"Finance domain knowledge base\"}\n",
    ")\n",
    "\n",
    "print(\"‚úÖ ChromaDB initialized\\n\")\n",
    "\n",
    "# Ingest finance documents\n",
    "doc_directory = \"./sample_docs/finance\"\n",
    "doc_path = Path(doc_directory)\n",
    "doc_files = list(doc_path.glob(\"*.txt\"))\n",
    "\n",
    "print(f\"üìö Found {len(doc_files)} finance documents:\\n\")\n",
    "for f in doc_files:\n",
    "    print(f\"  üìÑ {f.stem.replace('_', ' ').title()}\")\n",
    "\n",
    "print(f\"\\n{'='*100}\")\n",
    "print(\"INGESTING DOCUMENTS (Creating embeddings - this costs tokens!)\")\n",
    "print(f\"{'='*100}\\n\")\n",
    "\n",
    "all_ids = []\n",
    "all_chunks = []\n",
    "all_embeddings = []\n",
    "all_metadata = []\n",
    "chunk_counter = 0\n",
    "\n",
    "for doc_file in doc_files:\n",
    "    title = doc_file.stem.replace('_', ' ').title()\n",
    "    content = doc_file.read_text()\n",
    "\n",
    "    # Chunk the document\n",
    "    chunks = chunk_document(content, sentences_per_chunk=4, overlap_sentences=1)\n",
    "\n",
    "    print(f\"üìÑ {title}\")\n",
    "    print(f\"   ‚Üí {len(chunks)} chunks created\")\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk_id = f\"{doc_file.stem}_chunk_{i}\"\n",
    "        embedding = get_embedding(chunk)\n",
    "\n",
    "        all_ids.append(chunk_id)\n",
    "        all_chunks.append(chunk)\n",
    "        all_embeddings.append(embedding)\n",
    "        all_metadata.append({\n",
    "            \"title\": title,\n",
    "            \"category\": \"finance\",\n",
    "            \"document_type\": doc_file.stem,\n",
    "            \"chunk_index\": i,\n",
    "            \"total_chunks\": len(chunks)\n",
    "        })\n",
    "\n",
    "        chunk_counter += 1\n",
    "\n",
    "# Batch add to collection\n",
    "collection.add(\n",
    "    ids=all_ids,\n",
    "    documents=all_chunks,\n",
    "    embeddings=all_embeddings,\n",
    "    metadatas=all_metadata\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*100}\")\n",
    "print(f\"‚úÖ INGESTION COMPLETE!\")\n",
    "print(f\"{'='*100}\")\n",
    "print(f\"üìä Total chunks: {chunk_counter}\")\n",
    "print(f\"üí∞ Embedding cost: ~${chunk_counter * 0.00002:.6f}\")\n",
    "print(f\"üíæ Saved to: ./finance_notebook_db\")\n",
    "print(f\"\\nüéØ Now you can query unlimited times in Cell 4 without additional embedding costs!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Query the System (Run Multiple Times!)\n",
    "\n",
    "**Change the `question` variable and re-run** to explore different queries.\n",
    "\n",
    "No additional embedding costs - we already have everything stored! üéâ\n",
    "\n",
    "### Try These Questions:\n",
    "- \"How does a Loan Origination System handle income verification?\"\n",
    "- \"What is debt-to-credit ratio and how does it affect credit scores?\"\n",
    "- \"How does an LMS handle delinquent loans?\"\n",
    "- \"What are the DTI requirements for conventional mortgages?\"\n",
    "- \"How do credit scores impact loan approval?\"\n",
    "- \"What is the difference between hard and soft credit inquiries?\"\n",
    "- \"How are escrow accounts managed in loan servicing?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß CHANGE THIS QUESTION AND RE-RUN!\n",
    "question = \"How does a Loan Origination System handle income verification?\"\n",
    "\n",
    "# Number of chunks to retrieve (adjust based on question complexity)\n",
    "n_results = 3\n",
    "\n",
    "print(f\"{'='*100}\")\n",
    "print(f\"‚ùì QUESTION: {question}\")\n",
    "print(f\"{'='*100}\")\n",
    "\n",
    "# Step 1: Retrieve relevant chunks\n",
    "print(f\"\\nüîç Retrieving top {n_results} relevant chunks...\\n\")\n",
    "\n",
    "query_embedding = get_embedding(question)\n",
    "\n",
    "results = collection.query(\n",
    "    query_embeddings=[query_embedding],\n",
    "    n_results=n_results,\n",
    "    include=[\"documents\", \"metadatas\", \"distances\"]\n",
    ")\n",
    "\n",
    "# Format results\n",
    "retrieved_chunks = []\n",
    "for doc, meta, dist in zip(results[\"documents\"][0], results[\"metadatas\"][0], results[\"distances\"][0]):\n",
    "    similarity = 1 / (1 + dist)\n",
    "    retrieved_chunks.append({\n",
    "        \"text\": doc,\n",
    "        \"title\": meta[\"title\"],\n",
    "        \"document_type\": meta[\"document_type\"],\n",
    "        \"similarity\": similarity\n",
    "    })\n",
    "\n",
    "# Display retrieved chunks\n",
    "print_retrieved_chunks(retrieved_chunks)\n",
    "\n",
    "# Step 2: Generate answer with context\n",
    "print(f\"\\n{'='*100}\")\n",
    "print(\"ü§ñ GENERATING ANSWER...\")\n",
    "print(f\"{'='*100}\\n\")\n",
    "\n",
    "# Build context from retrieved chunks\n",
    "context_parts = []\n",
    "sources = set()\n",
    "\n",
    "for i, chunk in enumerate(retrieved_chunks, 1):\n",
    "    context_parts.append(f\"[Source {i}: {chunk['title']}]\\n{chunk['text']}\\n\")\n",
    "    sources.add(chunk['title'])\n",
    "\n",
    "context = \"\\n\".join(context_parts)\n",
    "\n",
    "# Construct prompt\n",
    "prompt = f\"\"\"You are a finance domain expert assistant specializing in lending systems, credit analysis, and loan processing.\n",
    "\n",
    "CONTEXT FROM KNOWLEDGE BASE:\n",
    "{context}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Answer the question using ONLY the information in the context above\n",
    "- If the context doesn't contain enough information, say so clearly\n",
    "- Cite your sources by mentioning the document titles\n",
    "- Be precise with financial terms and regulatory requirements\n",
    "- Use industry terminology appropriately\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "ANSWER:\"\"\"\n",
    "\n",
    "# Call LLM\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    temperature=0.2,  # Low temperature for factual finance answers\n",
    ")\n",
    "\n",
    "answer = response.choices[0].message.content\n",
    "tokens_used = response.usage.total_tokens\n",
    "cost = tokens_used * 0.00000015  # gpt-4o-mini pricing\n",
    "\n",
    "# Display answer\n",
    "print(f\"{'='*100}\")\n",
    "print(\"üí° ANSWER:\")\n",
    "print(f\"{'='*100}\")\n",
    "print(answer)\n",
    "print(f\"\\n{'='*100}\")\n",
    "print(f\"üìö Sources: {', '.join(sources)}\")\n",
    "print(f\"üìä Tokens used: {tokens_used} | Cost: ${cost:.6f}\")\n",
    "print(f\"{'='*100}\")\n",
    "\n",
    "print(\"\\n‚ú® Try changing the question above and re-running this cell!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Explore the Knowledge Base\n",
    "\n",
    "See what documents and chunks are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collection stats\n",
    "total_chunks = collection.count()\n",
    "\n",
    "print(f\"{'='*100}\")\n",
    "print(\"üìä KNOWLEDGE BASE STATISTICS\")\n",
    "print(f\"{'='*100}\")\n",
    "print(f\"Total chunks: {total_chunks}\")\n",
    "print(f\"Storage location: ./finance_notebook_db\")\n",
    "\n",
    "# Get sample chunks from each document type\n",
    "print(f\"\\n{'='*100}\")\n",
    "print(\"üìÑ SAMPLE CHUNKS BY DOCUMENT TYPE\")\n",
    "print(f\"{'='*100}\\n\")\n",
    "\n",
    "doc_types = [\"loan_origination_system\", \"loan_management_system\", \"credit_reports\", \"underwriting_guidelines\"]\n",
    "\n",
    "for doc_type in doc_types:\n",
    "    results = collection.get(\n",
    "        where={\"document_type\": doc_type},\n",
    "        limit=1,\n",
    "        include=[\"documents\", \"metadatas\"]\n",
    "    )\n",
    "    \n",
    "    if results['documents']:\n",
    "        meta = results['metadatas'][0]\n",
    "        doc = results['documents'][0]\n",
    "        \n",
    "        print(f\"üìÑ {meta['title']}\")\n",
    "        print(f\"   Total chunks: {meta['total_chunks']}\")\n",
    "        print(f\"   Sample: {doc[:150]}...\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Compare Different Retrieval Strategies\n",
    "\n",
    "Experiment with different numbers of chunks retrieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_question = \"What are the key components of underwriting?\"\n",
    "\n",
    "print(f\"Testing question: '{test_question}'\\n\")\n",
    "print(f\"{'='*100}\\n\")\n",
    "\n",
    "for n in [1, 3, 5]:\n",
    "    print(f\"üìä Retrieving TOP {n} chunks:\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    query_embedding = get_embedding(test_question)\n",
    "    \n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=n,\n",
    "        include=[\"metadatas\", \"distances\"]\n",
    "    )\n",
    "    \n",
    "    for i, (meta, dist) in enumerate(zip(results[\"metadatas\"][0], results[\"distances\"][0]), 1):\n",
    "        similarity = 1 / (1 + dist)\n",
    "        print(f\"  {i}. [{meta['title']}] - Similarity: {similarity:.3f}\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "print(f\"{'='*100}\")\n",
    "print(\"üí° OBSERVATION:\")\n",
    "print(\"More chunks = more context but also more noise\")\n",
    "print(\"Sweet spot is usually 3-5 chunks for most questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Test Edge Cases\n",
    "\n",
    "What happens when we ask questions NOT in the knowledge base?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question NOT in knowledge base\n",
    "out_of_scope_question = \"How do I deploy a React application to AWS?\"\n",
    "\n",
    "print(f\"‚ùì Out-of-scope question: '{out_of_scope_question}'\\n\")\n",
    "\n",
    "query_embedding = get_embedding(out_of_scope_question)\n",
    "\n",
    "results = collection.query(\n",
    "    query_embeddings=[query_embedding],\n",
    "    n_results=3,\n",
    "    include=[\"documents\", \"metadatas\", \"distances\"]\n",
    ")\n",
    "\n",
    "print(\"üìä Best matches (even though irrelevant):\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "for i, (doc, meta, dist) in enumerate(zip(results[\"documents\"][0], results[\"metadatas\"][0], results[\"distances\"][0]), 1):\n",
    "    similarity = 1 / (1 + dist)\n",
    "    print(f\"\\n{i}. [{meta['title']}] - Similarity: {similarity:.3f}\")\n",
    "    print(f\"   {doc[:150]}...\")\n",
    "\n",
    "print(f\"\\n{'='*100}\")\n",
    "print(\"üí° IMPORTANT: Notice the similarity scores are much lower!\")\n",
    "print(\"In production, set a similarity threshold (e.g., 0.4)\")\n",
    "print(\"If all results are below threshold, return 'I don't have information about that'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Key Takeaways\n",
    "\n",
    "### What You've Learned:\n",
    "\n",
    "1. **Domain-Specific RAG**: Finance documents (LOS, LMS, Credit Reports, Underwriting) can be queried semantically\n",
    "\n",
    "2. **Cost Efficiency**: Embed once (Cell 3), query unlimited times (Cell 4) - saves money!\n",
    "\n",
    "3. **Semantic Search**: Finds relevant information even with different terminology\n",
    "   - Ask about \"vacation\" ‚Üí finds \"PTO\"\n",
    "   - Ask about \"DTI\" ‚Üí finds \"debt-to-income ratio\"\n",
    "\n",
    "4. **Source Attribution**: Every answer cites sources for auditability (critical in finance!)\n",
    "\n",
    "5. **Retrieval Strategies**: \n",
    "   - More chunks = more context but potential noise\n",
    "   - Sweet spot: 3-5 chunks\n",
    "   - Use similarity thresholds to detect out-of-scope questions\n",
    "\n",
    "### Production Considerations for Finance:\n",
    "\n",
    "- **Compliance**: Audit logs for all queries (who asked what, when)\n",
    "- **Version Control**: Track which version of regulations/policies was used\n",
    "- **Metadata Filtering**: Filter by effective date, regulation type, jurisdiction\n",
    "- **Confidence Scores**: Flag low-confidence answers for human review\n",
    "- **Access Control**: Role-based access to different document types\n",
    "- **Regulatory Updates**: Easy document refresh without system changes\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. Add your own finance documents (replace sample_docs/finance)\n",
    "2. Experiment with chunk sizes (Cell 3: `sentences_per_chunk`)\n",
    "3. Try different retrieval amounts (Cell 4: `n_results`)\n",
    "4. Implement metadata filtering for your use case\n",
    "5. Build a production API with FastAPI\n",
    "\n",
    "üéØ **You now understand RAG for Finance domain!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
